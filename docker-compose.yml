version: "3.3"

x-base_service: &base_service
  stop_signal: SIGTERM
  tty: true
  stdin_open: true
  env_file: .env
  ipc: host

x-base_service_nvidia: &base_service_nvidia
  <<: *base_service
  profiles: ["nvidia", "nvidia_ingest"]
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            device_ids: ["0"]
            capabilities: [gpu]

x-base_service_amd: &base_service_amd
  <<: *base_service
  profiles: ["amd", "amd_ingest"]
  group_add:
    - video
  devices:
    - "/dev/dri"
    - "/dev/kfd"

x-base_service_vllm: &base_vllm
  ports:
    - "${VLLM_HOST_PORT}:${VLLM_CONT_PORT}"

x-base_service_ui: &base_ui
  ports:
    - "${UI_HOST_PORT}:${UI_CONT_PORT}"

x-base_service_omnipedia: &base_omnipedia
  ports:
    - "${OMNIPEDIA_HOST_PORT}:${OMNIPEDIA_CONT_PORT}"
  volumes:
    - ./rust/db:/db
    - ./rust/prompt:/prompt

x-base_service_omnipedia_ingest: &base_omnipedia_ingest
  ports:
    - "${OMNIPEDIA_HOST_PORT}:${OMNIPEDIA_CONT_PORT}"
  volumes:
    - ./wikidump:/wikidump

x-base_service_embeddings: &base_embeddings
  ports:
    - "${EMBED_HOST_PORT}:${EMBED_CONT_PORT}"

x-base_service_infinity: &base_infinity
  ports:
    - "${EMBED_HOST_PORT}:${EMBED_CONT_PORT}"

services:
  ui:
    <<:
      - *base_service
      - *base_ui
    build:
      dockerfile: Dockerfile
      context: ./typescript

  vllm-nv:
    <<:
      - *base_service_nvidia
      - *base_vllm
    build:
      dockerfile: Dockerfile_NVIDIA
      context: ./vllm
      args:
        TORCH_CUDA_ARCH_LIST: ${TORCH_CUDA_ARCH_LIST}
        LLM_MODEL_NAME: ${LLM_MODEL_NAME}

  omnipedia:
    <<:
      - *base_service
      - *base_omnipedia
    profiles: ["nvidia"]
    build:
      dockerfile: Dockerfile
      context: ./rust
      args:
        LLM_MODEL_NAME: ${LLM_MODEL_NAME}
  #   <<:
  #     - *base_service
  #     - *base_omnipedia_ingest
  #   profiles: ["nvidia_ingest"]
  #   build:
  #     dockerfile: Dockerfile_INGEST
  #     context: ./rust

  embeddings-nv:
    <<:
      - *base_service_nvidia
      - *base_embeddings
    build:
      dockerfile: Dockerfile_NVIDIA
      context: ./embeddings_service
      args:
        SBERT_MODEL_NAME: ${SBERT_MODEL_NAME}

  infinity-nv:
    <<:
      - *base_service_nvidia
      - *base_infinity
    build:
      dockerfile: Dockerfile_NVIDIA
      context: ./infinity
      args:
        SBERT_MODEL_NAME: ${SBERT_MODEL_NAME}

  vllm-amd:
    <<:
      - *base_service_amd
      - *base_vllm
    build:
      dockerfile: Dockerfile_AMD
      context: ./vllm

  embeddings-amd:
    <<:
      - *base_service_amd
      - *base_embeddings
    build:
      dockerfile: Dockerfile_AMD
      context: ./embeddings_service
