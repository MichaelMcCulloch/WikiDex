version: "3.3"

x-base_service: &base_service
  stop_signal: SIGTERM
  tty: true
  stdin_open: true
  env_file: .env
  ipc: host

x-base_service_nvidia: &base_service_nvidia
  <<: *base_service
  profiles: ["server"]
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            device_ids: ["0"]
            capabilities: [gpu]

x-base_ingest_service_nvidia: &base_ingest_nvidia_ingest
  <<: *base_service
  profiles: ["ingest"]
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            device_ids: ["0"]
            capabilities: [gpu]

# x-base_service_amd: &base_service_amd
#   <<: *base_service
#   profiles: ["amd"]
#   group_add:
#     - video
#   devices:
#     - "/dev/dri"
#     - "/dev/kfd"

x-base_service_vllm: &base_vllm
  ports:
    - "${VLLM_HOST_PORT}:${VLLM_CONT_PORT}"

x-base_service_ui: &base_ui
  ports:
    - "${UI_HOST_PORT}:${UI_CONT_PORT}"

x-base_service_omnipedia: &base_omnipedia
  ports:
    - "${OMNIPEDIA_HOST_PORT}:${OMNIPEDIA_CONT_PORT}"
  volumes:
    - ./rust/db:/db
    - ./rust/prompt:/prompt

x-base_service_omnipedia_ingest: &base_omnipedia_ingest
  ports:
    - "${OMNIPEDIA_HOST_PORT}:${OMNIPEDIA_CONT_PORT}"
  volumes:
    - ./wikidump:/wikidump

x-base_service_infinity: &base_infinity
  ports:
    - "${EMBED_HOST_PORT}:${EMBED_CONT_PORT}"

services:
  ui:
    <<:
      - *base_service
      - *base_ui
    profiles: ["server"]
    build:
      dockerfile: Dockerfile
      context: ./typescript

  vllm:
    <<:
      - *base_service_nvidia
      - *base_vllm
    profiles: ["server", "breeder"]
    build:
      dockerfile: Dockerfile
      context: ./vllm
      args:
        TORCH_CUDA_ARCH_LIST: ${TORCH_CUDA_ARCH_LIST}
        LLM_MODEL_NAME: ${LLM_MODEL_NAME}

  omnipedia:
    <<:
      - *base_service
      - *base_omnipedia
    profiles: ["server"]
    build:
      dockerfile: Dockerfile
      context: ./rust
      args:
        LLM_MODEL_NAME: ${LLM_MODEL_NAME}

  omnipedia-breeder:
    <<:
      - *base_service
      - *base_omnipedia
    profiles: ["breeder"]
    build:
      dockerfile: Dockerfile_Breeder
      context: ./rust
      args:
        LLM_MODEL_NAME: ${LLM_MODEL_NAME}

  infinity:
    <<:
      - *base_service_nvidia
      - *base_infinity
    profiles: ["server", "breeder"]
    environment:
      - BATCH_SIZE=128
    build:
      dockerfile: Dockerfile
      context: ./infinity
      args:
        SBERT_MODEL_NAME: ${SBERT_MODEL_NAME}

  infinity-ingest:
    <<:
      - *base_ingest_nvidia_ingest
      - *base_infinity
    profiles: ["ingest"]
    environment:
      - BATCH_SIZE=4096
    build:
      dockerfile: Dockerfile
      context: ./infinity
      args:
        SBERT_MODEL_NAME: ${SBERT_MODEL_NAME}
