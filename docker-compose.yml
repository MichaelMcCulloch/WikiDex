version: "3.3"
services:
  exllamav2_api:
    build:
      context: .
      args:
        TORCH_CUDA_ARCH_LIST: ${TORCH_CUDA_ARCH_LIST:-8.6}
    ports:
      - "${HOST_API_PORT:-5050}:${CONTAINER_API_PORT:-5050}"
    stdin_open: true
    tty: true
    volumes:
      - ./models:/app/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
