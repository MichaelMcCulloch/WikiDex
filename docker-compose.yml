x-base_service: &base_service
  stop_signal: SIGINT
  restart: always
  tty: true
  stdin_open: true
  env_file: .env
  ipc: host

x-base_service_nvidia: &base_service_nvidia
  <<: *base_service
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            device_ids: ["0"]
            capabilities: [gpu]

# x-base_service_amd: &base_service_amd
#   <<: *base_service
#
#   group_add:
#     - video
#   devices:
#     - "/dev/dri"
#     - "/dev/kfd"

x-base_service_vllm: &base_vllm
  env_file: .env
  image: vllm/vllm-openai:latest
  profiles:
    - vllm
  volumes:
    - ~/.cache/huggingface/:/root/.cache/huggingface
  ports:
    - "${VLLM_HOST_PORT}:${VLLM_CONT_PORT}"

x-base_service_triton: &base_triton
  env_file: .env
  image: triton/triton-openai:latest
  profiles:
    - triton
  build:
    context: ./triton
    dockerfile: Dockerfile
  volumes:
    - ~/.cache/huggingface/hub/:/root/.cache/huggingface/hub
    - ./triton/all_models:/all_models
    - ./triton/scripts:/opt/scripts
  ports:
    - 8000:8000
    - 8001:8001
    - 8002:8002

x-base_service_ui: &base_ui
  ports:
    - "${UI_HOST_PORT}:${UI_CONT_PORT}"

x-base_service_wikidex: &base_wikidex
  ports:
    - "${WIKIDEX_HOST_PORT}:${WIKIDEX_CONT_PORT}"
  volumes:
    - ./wikidex/prompt:/prompt

x-base_service_index: &base_index
  ports:
    - "${INDEX_HOST_PORT}:${INDEX_CONT_PORT}"
  volumes:
    - /home/michael/Documents/WIKIDUMPS/${EXTRACT_DATE}/index/thenlper/gte-small:/db

x-base_rust: &base_rust
  environment:
    RUST_LOG: $RUST_LOG

x-base_service_redis: &base_redis
  image: redis:latest
  command: >
    --requirepass redis
  ports:
    - $REDIS_PORT:$REDIS_PORT
  volumes:
    - /home/michael/ContainerData/Redis/wikidex/data:/root/redis
    - /home/michael/ContainerData/Redis/wikidex/redis.conf:/usr/local/etc/redis/redis.conf

x-base_service_postgres: &base_postgres
  image: postgres
  shm_size: 1gb

x-base_service_postgres_prod: &base_postgres_prod
  <<:
    - *base_postgres
  volumes:
    - /home/michael/ContainerData/PostgreSQL/wikidex/${EXTRACT_DATE}:/var/lib/postgresql/data
  ports:
    - $POSTGRES_PORT:$POSTGRES_PORT

x-base_service_postgres_dummy: &base_postgres_dummy
  <<:
    - *base_postgres
  volumes:
    - /home/michael/ContainerData/PostgreSQL/wikidex/dummy/:/var/lib/postgresql/data
  ports:
    - $POSTGRES_DUMMY_PORT:$POSTGRES_PORT

x-base_service_postgres_prod_gid: &base_postgres_prod_gid_fix
  <<:
    - *base_postgres_prod
  user: 1000:1000

x-base_service_postgres_dummy_gid: &base_postgres_dummy_gid_fix
  <<:
    - *base_postgres_dummy
  user: 1000:1000

x-base_service_infinity: &base_infinity
  image: michaelf34/infinity:latest

  volumes:
    - ~/.cache/huggingface/hub/:/app/.cache/torch
  ports:
    - "${EMBED_HOST_PORT}:${EMBED_CONT_PORT}"

services:
  infinity:
    <<:
      - *base_service_nvidia
      - *base_infinity
    entrypoint:
      [
        infinity_emb,
        --url-prefix,
        "/v1",
        --model-name-or-path,
        "$SBERT_MODEL_NAME",
        --port,
        "$EMBED_CONT_PORT",
        --device,
        cuda,
        --batch-size,
        "$BATCH_SIZE",
      ]

  ui:
    <<:
      - *base_service
      - *base_ui
    build:
      dockerfile: Dockerfile
      context: ./typescript

  vllm:
    <<:
      - *base_service_nvidia
      - *base_vllm
    entrypoint:
      [
        python3,
        -m,
        vllm.entrypoints.openai.api_server,
        --max-log-len,
        "0",
        --disable-log-stats,
        --disable-log-requests,
        --gpu-memory-utilization,
        $VLLM_MEM_MAX,
        --model,
        $LLM_MODEL_NAME,
        --quantization,
        $QUANTIZATION,
        --host,
        "0.0.0.0",
        --port,
        $VLLM_CONT_PORT,
        --dtype,
        half,
        --max-model-len,
        $MAX_MODEL_LEN,
      ]

  triton:
    <<:
      - *base_service_nvidia
      - *base_triton
    entrypoint:
      [
        python3,
        /opt/scripts/launch_triton_server.py,
        --model_repo,
        /all_models/inflight_batcher_llm,
        --tensorrt_llm_model_name,
        mistral,
        --world_size,
        "1",
      ]

  redis:
    <<:
      - *base_service
      - *base_redis

  postgres:
    <<:
      - *base_service
      - *base_postgres_prod_gid_fix

  postgres_dummy:
    profiles:
      - dummy
    <<:
      - *base_service
      - *base_postgres_dummy_gid_fix

  wikidex:
    <<:
      - *base_service
      - *base_wikidex
      - *base_rust
    build:
      dockerfile: Dockerfile
      context: ./wikidex
    depends_on:
      - postgres

  index:
    <<:
      - *base_service
      - *base_index
      - *base_rust
    build: https://github.com/MichaelMcCulloch/face.git#0.1.1
    entrypoint: [face, --index-path, $INDEX_PATH]
