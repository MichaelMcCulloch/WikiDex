x-base_service: &base_service
  stop_signal: SIGINT
  restart: always
  tty: true
  stdin_open: true
  env_file: .env
  ipc: host

x-base_service_nvidia: &base_service_nvidia
  <<: *base_service
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            device_ids: ["0"]
            capabilities: [gpu]

x-base_service_triton: &base_triton
  env_file: .env
  profiles:
    - triton
    - ingest
  build:
    context: ../../../triton
    dockerfile: Dockerfile
  volumes:
    - ~/.cache/huggingface/hub/:/root/.cache/huggingface/hub
    - ~/.cache/TensorRT/TritonConfigs/${TRITON_ENGINE_NAME}/:/all_models/inflight_batcher_llm/
    - ~/.cache/TensorRT/Engine/${TRITON_ENGINE_NAME}-128x32768/:/all_models/inflight_batcher_llm/tensorrt_llm/1/
  ports:
    - 8000:8000
    - 8001:8001
    - 8002:8002
services:
  triton:
    <<:
      - *base_service_nvidia
      - *base_triton
    entrypoint:
      [
        python3,
        /opt/scripts/launch_triton_server.py,
        --model_repo,
        /all_models/inflight_batcher_llm,
        --tensorrt_llm_model_name,
        mistral,
        --world_size,
        "1",
      ]
    depends_on:
      - infinity
