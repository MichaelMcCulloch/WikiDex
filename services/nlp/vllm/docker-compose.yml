x-base_service_vllm: &base_vllm
  env_file: .env
  image: vllm/vllm-openai:latest
  profiles:
    - vllm
  volumes:
    - ~/.cache/huggingface/:/root/.cache/huggingface
  ports:
    - "${VLLM_HOST_PORT}:${VLLM_CONT_PORT}"

x-base_service: &base_service
  stop_signal: SIGINT
  restart: always
  tty: true
  stdin_open: true
  env_file: .env
  ipc: host

x-base_service_nvidia: &base_service_nvidia
  <<: *base_service
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            device_ids: ["0"]
            capabilities: [gpu]

services:
  vllm:
    <<:
      - *base_service_nvidia
      - *base_vllm
    entrypoint:
      [
        python3,
        -m,
        vllm.entrypoints.openai.api_server,
        --max-log-len,
        "0",
        --disable-log-stats,
        --disable-log-requests,
        --gpu-memory-utilization,
        $VLLM_MEM_MAX,
        --model,
        $LLM_MODEL_NAME,
        --quantization,
        $QUANTIZATION,
        --host,
        "0.0.0.0",
        --port,
        $VLLM_CONT_PORT,
        --dtype,
        half,
        --max-model-len,
        $MAX_MODEL_LEN,
      ]
